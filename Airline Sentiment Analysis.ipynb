{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d1939c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a92430ae",
   "metadata": {},
   "source": [
    "Read the training using pandas module and select only the sentiment and text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d4c8bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the data set \n",
    "import pandas as pd \n",
    "train = pd.read_csv('Tweets-train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3002801e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570301130888122368</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.3486</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:59 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>570301083672813571</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6837</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>yvonnalynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:48 -0800</td>\n",
       "      <td>Lets Play</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>570301031407624196</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.7033</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:15:36 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>570300817074462722</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Can't Tell</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jnardino</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:14:45 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570300616901320704</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6745</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>Virgin America</td>\n",
       "      <td>NaN</td>\n",
       "      <td>cjmcginnis</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 11:13:57 -0800</td>\n",
       "      <td>San Francisco CA</td>\n",
       "      <td>Pacific Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570301130888122368          positive                        0.3486   \n",
       "1  570301083672813571           neutral                        0.6837   \n",
       "2  570301031407624196          negative                        1.0000   \n",
       "3  570300817074462722          negative                        1.0000   \n",
       "4  570300616901320704          positive                        0.6745   \n",
       "\n",
       "  negativereason  negativereason_confidence         airline  \\\n",
       "0            NaN                     0.0000  Virgin America   \n",
       "1            NaN                        NaN  Virgin America   \n",
       "2     Bad Flight                     0.7033  Virgin America   \n",
       "3     Can't Tell                     1.0000  Virgin America   \n",
       "4            NaN                     0.0000  Virgin America   \n",
       "\n",
       "  airline_sentiment_gold        name negativereason_gold  retweet_count  \\\n",
       "0                    NaN    jnardino                 NaN              0   \n",
       "1                    NaN  yvonnalynn                 NaN              0   \n",
       "2                    NaN    jnardino                 NaN              0   \n",
       "3                    NaN    jnardino                 NaN              0   \n",
       "4                    NaN  cjmcginnis                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  @VirginAmerica plus you've added commercials t...         NaN   \n",
       "1  @VirginAmerica I didn't today... Must mean I n...         NaN   \n",
       "2  @VirginAmerica it's really aggressive to blast...         NaN   \n",
       "3  @VirginAmerica and it's a really big bad thing...         NaN   \n",
       "4  @VirginAmerica yes, nearly every time I fly VX...         NaN   \n",
       "\n",
       "               tweet_created    tweet_location               user_timezone  \n",
       "0  2015-02-24 11:15:59 -0800               NaN  Pacific Time (US & Canada)  \n",
       "1  2015-02-24 11:15:48 -0800         Lets Play  Central Time (US & Canada)  \n",
       "2  2015-02-24 11:15:36 -0800               NaN  Pacific Time (US & Canada)  \n",
       "3  2015-02-24 11:14:45 -0800               NaN  Pacific Time (US & Canada)  \n",
       "4  2015-02-24 11:13:57 -0800  San Francisco CA  Pacific Time (US & Canada)  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcff190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10640 entries, 0 to 10639\n",
      "Data columns (total 15 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   tweet_id                      10640 non-null  int64  \n",
      " 1   airline_sentiment             10640 non-null  object \n",
      " 2   airline_sentiment_confidence  10640 non-null  float64\n",
      " 3   negativereason                6670 non-null   object \n",
      " 4   negativereason_confidence     7672 non-null   float64\n",
      " 5   airline                       10640 non-null  object \n",
      " 6   airline_sentiment_gold        31 non-null     object \n",
      " 7   name                          10640 non-null  object \n",
      " 8   negativereason_gold           26 non-null     object \n",
      " 9   retweet_count                 10640 non-null  int64  \n",
      " 10  text                          10640 non-null  object \n",
      " 11  tweet_coord                   760 non-null    object \n",
      " 12  tweet_created                 10640 non-null  object \n",
      " 13  tweet_location                7184 non-null   object \n",
      " 14  user_timezone                 7136 non-null   object \n",
      "dtypes: float64(2), int64(2), object(11)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0304b31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting only sentiment and tweet column from the entire data set\n",
    "train= train[['airline_sentiment','text']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7493c9f",
   "metadata": {},
   "source": [
    "‚Ä¢\tObserve randomly generated 10 tweets for each sentiment with respect to the following:\n",
    "o\tText contains references with ‚Äò@‚Äô\n",
    "o\tText contains links (http , https )\n",
    "o\tText contains punctuations\n",
    "o\tText contains Emoticons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d74deee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@USAirways it was customer service like I have never seen before!  Kudos to your organization.\n",
      "\n",
      "@AmericanAir I love your company and your staff is amazing. They just made an uncomfortable situation comfortable\n",
      "\n",
      "@united I appreciate the follow up.\n",
      "\n",
      "@JetBlue boarding the back of the airplane first. Like a boss. #sosmart #jetblue #frequentflyerappreciates #alsoyayforsnacks\n",
      "\n",
      "@AmericanAir mission accomplished today, Thank you!\n",
      "\n",
      "@united thnx\n",
      "\n",
      "@united thank you for following up!\n",
      "\n",
      "@JetBlue thanks so much!! ‚ù§Ô∏è‚ú® very relaxing flight!\n",
      "\n",
      "@USAirways thanks for seating me next to 2 hot athletes. This flight is significantly better now!\n",
      "\n",
      "@united awesome new plane flight 1701\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see some positive sentiments \n",
    "for each in train[train['airline_sentiment']==\"positive\"].sample(10,random_state=10)['text']:\n",
    "    print(each)\n",
    "    print()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2fea557",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@AmericanAir continues to win: I've never missed a flight before, but a nice little quiet gate change made it possible. Sheesh.\n",
      "\n",
      "@united is that all that matters, not the fact that we're at a different destination, we were put through a tremendous amount of stress,\n",
      "\n",
      "@USAirways your lack of customer service has shined. I need you to step up and get my lost baggage to delta. So they can return it to me.\n",
      "\n",
      "@united Terribly disappointed. Confirmed reservation delayed and your cust. service staff was not helpful in finding an alternate solution.\n",
      "\n",
      "@united what is this subtlety gate changes? Are you kidding with me?\n",
      "\n",
      "@SouthwestAir and now no wifi??? Come on.\n",
      "\n",
      "@JetBlue is flight 51 on 4/24/15 moved back? When I booked it said we arrive 11:31 but now it says 12:08 üò¢\n",
      "\n",
      "@AmericanAir complt incompetence on flt 295.Lav delay from a pln that lnded last nite, no internet and poor svc. Not what I expect from u.\n",
      "\n",
      "@united @annricord 0162431184663.\r\n",
      "3 of your agents said we would be refunded. Agents said United should never have sold us the ticket.\n",
      "\n",
      "@united I'm checked in, agent wouldn't tag my bags at 7am. Now I'm standing in line hell.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see some Negative sentiments \n",
    "for each in train[train['airline_sentiment']==\"negative\"].sample(10,random_state=10)['text']:\n",
    "    print(each)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37c75f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@JetBlue you guys get rid of the hip hop stations on Sirius XM?\n",
      "\n",
      "@JetBlue deal!\n",
      "\n",
      "@united can I request a ticket change through twitter ?\n",
      "\n",
      "@united My mom left her Kindle on flight 1544 today. Burgundy case with a light. Seat 27D. Did anyone find it?\n",
      "\n",
      "@USAirways am 2. 1/2 hours from airport sure would like to talk to someone\n",
      "\n",
      "@JetBlue well I'm not sure I'm that bold! lol or are you saying you didn't believe me?? :P\n",
      "\n",
      "@united can you send me another confirmation email?\n",
      "\n",
      "@SouthwestAir first time flyer, scheduled a (round)trip. set on departure date not sure on returning date, policy/fees on changing Re Flight\n",
      "\n",
      "@SouthwestAir still haven't been able to get through, thanks for responding\n",
      "\n",
      "@USAirways we even offered to fly in to another airport and they said they couldn't do that. No explanation why they can't.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# see some neutral  sentiments \n",
    "for each in train[train['airline_sentiment']==\"neutral\"].sample(10,random_state=10)['text']:\n",
    "    print(each)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8b67d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "## What are the observations ?#### 1.  Data contains words starting with '@'\n",
    "#### 2.  Data contains words having '#' \n",
    "#### 3.  Data contains links 'https:....\" \n",
    "#### 4. Data contains emoticons and punctuations such as ' , . ; ‚ù§Ô∏è‚ú® ! etc etc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cbcf1d",
   "metadata": {},
   "source": [
    " prepare a function to clean all the above observed tokens from the\n",
    "tweet text.\n",
    "Save changes in a new column\n",
    "\n",
    "The next step can be to clean the data and remove such things as they are not going to help in classifer model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0ef6fd",
   "metadata": {},
   "source": [
    "### https://regex101.com/\n",
    "### https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30a21ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@virginamerica Well, I didn't‚Ä¶but NOW I DO! :-D\n",
      "\n",
      "virginamerica Well, I didn't‚Ä¶but NOW I DO! :-D\n"
     ]
    }
   ],
   "source": [
    "# @ mentions \n",
    "import re \n",
    "print(train.text[5])\n",
    "print()\n",
    "print(re.sub(r'@+','',train.text[5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "90b0b9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel http://t.co/ahlXHhKiyn\n",
      "\n",
      "@VirginAmerica @virginmedia I'm flying your #fabulous #Seductive skies again! U take all the #stress away from travel .co/ahlXHhKiyn\n"
     ]
    }
   ],
   "source": [
    "# links \n",
    "print(train.text[10])\n",
    "print()\n",
    "print(re.sub('http?://[A-Za-z0-9./]','',train.text[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee3dd95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select any alphabets numbers so that punctuations and emoticons are removed \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "419d2f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@JetBlue thanks so much!! ‚ù§Ô∏è‚ú® very relaxing flight!\n",
      "\n",
      " JetBlue thanks so much       very relaxing flight \n"
     ]
    }
   ],
   "source": [
    "print (train.text[5977])\n",
    "print()\n",
    "print (re.sub(\"[^a-zA-Z0-9]\", \" \",train.text[5977]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d8f0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@VirginAmerica amazing to me that we can't get any cold air from the vents. #VX358 #noair #worstflightever #roasted #SFOtoBOS\n",
      "\n",
      " VirginAmerica amazing to me that we can t get any cold air from the vents   VX358  noair  worstflightever  roasted  SFOtoBOS\n"
     ]
    }
   ],
   "source": [
    "print (train.text[22])\n",
    "print() \n",
    "print (re.sub(\"[^a-zA-Z0-9]\", \" \",train.text[22]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1921bc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer \n",
    "tokenizer = WordPunctTokenizer()\n",
    "def tweet_cleaner(text):\n",
    "    text = re.sub(r'@+',' ',text)\n",
    "    text=re.sub('http?://[A-Za-z0-9./]','',text)\n",
    "    text=re.sub(\"[^a-zA-Z]\",\" \",text)\n",
    "    lower_case= text.lower()\n",
    "    words = tokenizer.tokenize(lower_case)\n",
    "    return (\" \".join(words)).strip()\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "20e5d8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Cleaned-Text']=list(map(lambda x:tweet_cleaner(x),train['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f1c06813",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>Cleaned-Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>virginamerica plus you ve added commercials to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>virginamerica i didn t today must mean i need ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>virginamerica it s really aggressive to blast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>virginamerica and it s a really big bad thing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "      <td>virginamerica yes nearly every time i fly vx t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  \\\n",
       "0          positive  @VirginAmerica plus you've added commercials t...   \n",
       "1           neutral  @VirginAmerica I didn't today... Must mean I n...   \n",
       "2          negative  @VirginAmerica it's really aggressive to blast...   \n",
       "3          negative  @VirginAmerica and it's a really big bad thing...   \n",
       "4          positive  @VirginAmerica yes, nearly every time I fly VX...   \n",
       "\n",
       "                                        Cleaned-Text  \n",
       "0  virginamerica plus you ve added commercials to...  \n",
       "1  virginamerica i didn t today must mean i need ...  \n",
       "2  virginamerica it s really aggressive to blast ...  \n",
       "3  virginamerica and it s a really big bad thing ...  \n",
       "4  virginamerica yes nearly every time i fly vx t...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d834762b",
   "metadata": {},
   "source": [
    "# List down the most common 15 words for each sentiment. Observe the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "506d871d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "[('to', 4326), ('i', 3321), ('the', 3022), ('a', 2351), ('flight', 2113), ('united', 2101), ('and', 2049), ('on', 2023), ('for', 1999), ('you', 1959), ('my', 1726), ('usairways', 1723), ('americanair', 1549), ('is', 1526), ('t', 1333)]\n",
      "\n",
      "neutral\n",
      "[('to', 1185), ('i', 1008), ('the', 730), ('a', 624), ('you', 561), ('jetblue', 540), ('united', 531), ('southwestair', 489), ('on', 479), ('for', 443), ('flight', 436), ('my', 392), ('is', 372), ('co', 366), ('americanair', 363)]\n",
      "\n",
      "positive\n",
      "[('the', 690), ('to', 675), ('you', 672), ('i', 555), ('for', 493), ('thanks', 448), ('jetblue', 443), ('southwestair', 424), ('a', 390), ('united', 376), ('thank', 336), ('and', 306), ('flight', 269), ('my', 263), ('americanair', 254)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import Counter\n",
    "for group_name,subset in train.groupby('airline_sentiment'):\n",
    "    sentimentData=subset['Cleaned-Text']\n",
    "    words=[]\n",
    "    for each in sentimentData:\n",
    "        words.extend(each.split(\" \"))\n",
    "    print (group_name)\n",
    "    print (Counter(words).most_common(15))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4c0064",
   "metadata": {},
   "source": [
    "We observe that most of the frequencies are of stopwords , so let's remove them "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f25c917",
   "metadata": {},
   "source": [
    "Remove Stopwords from all the tweets.\n",
    "Save changes in a new column and list down most common 15 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e1644463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Preprocess file ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f53dd5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\ndef RemoveStopWords(string):\\n    #Removing Punctuations\\n    for each in punctuation:\\n        string=string.replace(each,\"\")\\n    \\n    #Removing Stopwords\\n    english_stopwords=stopwords.words(\\'english\\')\\n    stopwords_removed_tokens=[]\\n    words=string.split(\" \")\\n    \\n    for each in words:\\n        if each not in english_stopwords:\\n            stopwords_removed_tokens.append(each)\\n    return \" \".join(stopwords_removed_tokens) \\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "def RemoveStopWords(string):\n",
    "    #Removing Punctuations\n",
    "    for each in punctuation:\n",
    "        string=string.replace(each,\"\")\n",
    "    \n",
    "    #Removing Stopwords\n",
    "    english_stopwords=stopwords.words('english')\n",
    "    stopwords_removed_tokens=[]\n",
    "    words=string.split(\" \")\n",
    "    \n",
    "    for each in words:\n",
    "        if each not in english_stopwords:\n",
    "            stopwords_removed_tokens.append(each)\n",
    "    return \" \".join(stopwords_removed_tokens) \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2da26047",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Preprocess  import RemoveStopWords   \n",
    "train['Clean-Text-StopWords-Removed']=list(map(lambda x:RemoveStopWords(x),train['Cleaned-Text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9462d3ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "[('flight', 2113), ('united', 2101), ('usairways', 1723), ('americanair', 1549), ('southwestair', 884), ('jetblue', 755), ('get', 728), ('cancelled', 648), ('service', 530), ('hours', 502), ('help', 443), ('customer', 428), ('time', 426), ('hold', 424), ('plane', 387)]\n",
      "\n",
      "neutral\n",
      "[('jetblue', 540), ('united', 531), ('southwestair', 489), ('flight', 436), ('co', 366), ('americanair', 363), ('usairways', 303), ('get', 173), ('please', 132), ('virginamerica', 130), ('flights', 130), ('help', 121), ('thanks', 115), ('need', 114), ('would', 92)]\n",
      "\n",
      "positive\n",
      "[('thanks', 448), ('jetblue', 443), ('southwestair', 424), ('united', 376), ('thank', 336), ('flight', 269), ('americanair', 254), ('usairways', 199), ('co', 176), ('great', 165), ('service', 120), ('virginamerica', 114), ('love', 105), ('best', 85), ('customer', 85)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Again let's see the counts of most common words after removing stopwords.\n",
    "\n",
    "from collections import Counter\n",
    "for group_name,subset in train.groupby('airline_sentiment'):\n",
    "    sentimentData=subset['Clean-Text-StopWords-Removed']\n",
    "    words=[]\n",
    "    for each in sentimentData:\n",
    "        words.extend(each.split(\" \"))\n",
    "    print (group_name)\n",
    "    print (Counter(words).most_common(15))\n",
    "    print ()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80572885",
   "metadata": {},
   "source": [
    "Remove these words from all the tweets.\n",
    "\n",
    "americanair, united, delta, southwestair, jetblue, virginamerica, usairways, flight, plane\n",
    "\n",
    "Save changes in a new column and list down most common 15 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6e426f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RemoveExplicitlyMentionedWords(string,listofWordsToRemove):\n",
    "    listofAllWords=string.split(\" \")\n",
    "    listofWords= [x for x in listofAllWords if x not in listofWordsToRemove]\n",
    "    return (\" \".join(listofWords)).strip()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e196d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_words_to_remove=['americanair','united','delta','southwestair','jetblue','virginamerica','usairways','flight','plane']\n",
    "train['Final-Wrangled-Text']=list(map(lambda x:RemoveExplicitlyMentionedWords(x,list_of_words_to_remove),train['Clean-Text-StopWords-Removed']))                         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7200e66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count again words in All sentiment \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0cfa6343",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "negative\n",
      "[('get', 728), ('cancelled', 648), ('service', 530), ('hours', 502), ('help', 443), ('customer', 428), ('time', 426), ('hold', 424), ('delayed', 371), ('amp', 368), ('us', 364), ('still', 363), ('call', 334), ('hour', 324), ('co', 323)]\n",
      "\n",
      "neutral\n",
      "[('co', 366), ('get', 173), ('please', 132), ('flights', 130), ('help', 121), ('thanks', 115), ('need', 114), ('would', 92), ('dm', 92), ('time', 80), ('tomorrow', 76), ('cancelled', 74), ('amp', 73), ('know', 72), ('us', 72)]\n",
      "\n",
      "positive\n",
      "[('thanks', 448), ('thank', 336), ('co', 176), ('great', 165), ('service', 120), ('love', 105), ('best', 85), ('customer', 85), ('good', 82), ('guys', 81), ('much', 77), ('get', 76), ('awesome', 71), ('got', 71), ('help', 65)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for group_name,subset in train.groupby('airline_sentiment'):\n",
    "    sentimentData=subset['Final-Wrangled-Text']\n",
    "    words=[]\n",
    "    for each in sentimentData:\n",
    "        words.extend(each.split(\" \"))\n",
    "    print (group_name)\n",
    "    print (Counter(words).most_common(15))\n",
    "    print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bd4e70",
   "metadata": {},
   "source": [
    "# Encode Sentiments using Label Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bed92334",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>Cleaned-Text</th>\n",
       "      <th>Clean-Text-StopWords-Removed</th>\n",
       "      <th>Final-Wrangled-Text</th>\n",
       "      <th>SentimentLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "      <td>virginamerica plus you ve added commercials to...</td>\n",
       "      <td>virginamerica plus added commercials experienc...</td>\n",
       "      <td>plus added commercials experience tacky</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
       "      <td>virginamerica i didn t today must mean i need ...</td>\n",
       "      <td>virginamerica today must mean need take anothe...</td>\n",
       "      <td>today must mean need take another trip</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "      <td>virginamerica it s really aggressive to blast ...</td>\n",
       "      <td>virginamerica really aggressive blast obnoxiou...</td>\n",
       "      <td>really aggressive blast obnoxious entertainmen...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "      <td>virginamerica and it s a really big bad thing ...</td>\n",
       "      <td>virginamerica really big bad thing</td>\n",
       "      <td>really big bad thing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  \\\n",
       "0          positive  @VirginAmerica plus you've added commercials t...   \n",
       "1           neutral  @VirginAmerica I didn't today... Must mean I n...   \n",
       "2          negative  @VirginAmerica it's really aggressive to blast...   \n",
       "3          negative  @VirginAmerica and it's a really big bad thing...   \n",
       "\n",
       "                                        Cleaned-Text  \\\n",
       "0  virginamerica plus you ve added commercials to...   \n",
       "1  virginamerica i didn t today must mean i need ...   \n",
       "2  virginamerica it s really aggressive to blast ...   \n",
       "3  virginamerica and it s a really big bad thing ...   \n",
       "\n",
       "                        Clean-Text-StopWords-Removed  \\\n",
       "0  virginamerica plus added commercials experienc...   \n",
       "1  virginamerica today must mean need take anothe...   \n",
       "2  virginamerica really aggressive blast obnoxiou...   \n",
       "3                 virginamerica really big bad thing   \n",
       "\n",
       "                                 Final-Wrangled-Text  SentimentLabel  \n",
       "0            plus added commercials experience tacky               2  \n",
       "1             today must mean need take another trip               1  \n",
       "2  really aggressive blast obnoxious entertainmen...               0  \n",
       "3                               really big bad thing               0  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder \n",
    "l = LabelEncoder()\n",
    "train['SentimentLabel']=l.fit_transform(train['airline_sentiment'])\n",
    "train.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bc4e82",
   "metadata": {},
   "source": [
    "# Here we observe \n",
    "0 - neutral \n",
    "1 - positive\n",
    "2 - negative "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42eafd4",
   "metadata": {},
   "source": [
    "Vectorize the Text Column (You can choose any vectorizer of your choice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "188dfe69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1d959bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = vectorizer.fit_transform(train['Final-Wrangled-Text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b710827",
   "metadata": {},
   "source": [
    "Prepare a multiclass Classification model using any classification algorithm and create a model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbf2029f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train  = train['SentimentLabel']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b0f856",
   "metadata": {},
   "source": [
    "Prepare Model Using Navie Bayes Classifier \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "254b85f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"‚ñ∏\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"‚ñæ\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics \n",
    "nb = MultinomialNB()\n",
    "nb.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f95660f",
   "metadata": {},
   "source": [
    "Read the test data and carry our data cleaning, encoding and vectorising operations on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "996d5277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>airline_sentiment_confidence</th>\n",
       "      <th>negativereason</th>\n",
       "      <th>negativereason_confidence</th>\n",
       "      <th>airline</th>\n",
       "      <th>airline_sentiment_gold</th>\n",
       "      <th>name</th>\n",
       "      <th>negativereason_gold</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_coord</th>\n",
       "      <th>tweet_created</th>\n",
       "      <th>tweet_location</th>\n",
       "      <th>user_timezone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>570031758546063361</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Customer Service Issue</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>msofka</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir why did you drop my call. Why don...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 17:25:36 -0800</td>\n",
       "      <td>Cleveland, OH</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>569230567759327233</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Bad Flight</td>\n",
       "      <td>0.6703</td>\n",
       "      <td>US Airways</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POnions</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@USAirways thanks for the seat that doesn't re...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-21 12:21:57 -0800</td>\n",
       "      <td>CT, USA</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>569847788986462209</td>\n",
       "      <td>negative</td>\n",
       "      <td>1.0000</td>\n",
       "      <td>Late Flight</td>\n",
       "      <td>0.6684</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>JoshSeefried</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir wasn't just a delay. Your counter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-23 05:14:34 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Central Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>569666352794898432</td>\n",
       "      <td>positive</td>\n",
       "      <td>0.6600</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>United</td>\n",
       "      <td>NaN</td>\n",
       "      <td>anoyes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@united social media team is on point on #Osca...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-22 17:13:37 -0800</td>\n",
       "      <td>San Francisco, CA</td>\n",
       "      <td>Eastern Time (US &amp; Canada)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>570279368766959616</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.6739</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>American</td>\n",
       "      <td>NaN</td>\n",
       "      <td>jjqb1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>@AmericanAir These birds could fly to South Am...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-02-24 09:49:31 -0800</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Buenos Aires</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
       "0  570031758546063361          negative                        1.0000   \n",
       "1  569230567759327233          negative                        1.0000   \n",
       "2  569847788986462209          negative                        1.0000   \n",
       "3  569666352794898432          positive                        0.6600   \n",
       "4  570279368766959616           neutral                        0.6739   \n",
       "\n",
       "           negativereason  negativereason_confidence     airline  \\\n",
       "0  Customer Service Issue                     1.0000    American   \n",
       "1              Bad Flight                     0.6703  US Airways   \n",
       "2             Late Flight                     0.6684    American   \n",
       "3                     NaN                     0.0000      United   \n",
       "4                     NaN                        NaN    American   \n",
       "\n",
       "  airline_sentiment_gold          name negativereason_gold  retweet_count  \\\n",
       "0                    NaN        msofka                 NaN              0   \n",
       "1                    NaN       POnions                 NaN              0   \n",
       "2                    NaN  JoshSeefried                 NaN              0   \n",
       "3                    NaN        anoyes                 NaN              0   \n",
       "4                    NaN         jjqb1                 NaN              0   \n",
       "\n",
       "                                                text tweet_coord  \\\n",
       "0  @AmericanAir why did you drop my call. Why don...         NaN   \n",
       "1  @USAirways thanks for the seat that doesn't re...         NaN   \n",
       "2  @AmericanAir wasn't just a delay. Your counter...         NaN   \n",
       "3  @united social media team is on point on #Osca...         NaN   \n",
       "4  @AmericanAir These birds could fly to South Am...         NaN   \n",
       "\n",
       "               tweet_created     tweet_location               user_timezone  \n",
       "0  2015-02-23 17:25:36 -0800      Cleveland, OH  Central Time (US & Canada)  \n",
       "1  2015-02-21 12:21:57 -0800            CT, USA  Central Time (US & Canada)  \n",
       "2  2015-02-23 05:14:34 -0800                NaN  Central Time (US & Canada)  \n",
       "3  2015-02-22 17:13:37 -0800  San Francisco, CA  Eastern Time (US & Canada)  \n",
       "4  2015-02-24 09:49:31 -0800                NaN                Buenos Aires  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('Tweets-test.csv')\n",
    "test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7cabddd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = test[['airline_sentiment','text']]\n",
    "test['Cleaned-Text']=list(map(lambda  x: tweet_cleaner(x),test['text']))\n",
    "test['Cleaned-Text-StopWords-Removed']=list(map(lambda x:RemoveStopWords(x),test['Cleaned-Text']))\n",
    "test['Final-Wrangled-Text']=list(map(lambda x:RemoveExplicitlyMentionedWords(x,list_of_words_to_remove),test['Cleaned-Text-StopWords-Removed'] ))\n",
    "x_test = vectorizer.transform(test['Final-Wrangled-Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "162b2cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding label for test data as well\n",
    "test['SentimentLabel']=l.transform(test['airline_sentiment'])\n",
    "\n",
    "y_test=test['SentimentLabel']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9944d30e",
   "metadata": {},
   "source": [
    "Predict the sentiments for test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "834ddc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = nb.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b593d1eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "      <th>Cleaned-Text</th>\n",
       "      <th>Cleaned-Text-StopWords-Removed</th>\n",
       "      <th>Final-Wrangled-Text</th>\n",
       "      <th>SentimentLabel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir why did you drop my call. Why don...</td>\n",
       "      <td>americanair why did you drop my call why don t...</td>\n",
       "      <td>americanair drop call people answering phones ...</td>\n",
       "      <td>drop call people answering phones always high ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@USAirways thanks for the seat that doesn't re...</td>\n",
       "      <td>usairways thanks for the seat that doesn t rec...</td>\n",
       "      <td>usairways thanks seat recline shocked asked se...</td>\n",
       "      <td>thanks seat recline shocked asked serve everyo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@AmericanAir wasn't just a delay. Your counter...</td>\n",
       "      <td>americanair wasn t just a delay your counter w...</td>\n",
       "      <td>americanair delay counter take valid cac card ...</td>\n",
       "      <td>delay counter take valid cac card valid id nee...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>positive</td>\n",
       "      <td>@united social media team is on point on #Osca...</td>\n",
       "      <td>united social media team is on point on oscarn...</td>\n",
       "      <td>united social media team point oscarnight</td>\n",
       "      <td>social media team point oscarnight</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neutral</td>\n",
       "      <td>@AmericanAir These birds could fly to South Am...</td>\n",
       "      <td>americanair these birds could fly to south ame...</td>\n",
       "      <td>americanair birds could fly south america exam...</td>\n",
       "      <td>birds could fly south america example argentina</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text  \\\n",
       "0          negative  @AmericanAir why did you drop my call. Why don...   \n",
       "1          negative  @USAirways thanks for the seat that doesn't re...   \n",
       "2          negative  @AmericanAir wasn't just a delay. Your counter...   \n",
       "3          positive  @united social media team is on point on #Osca...   \n",
       "4           neutral  @AmericanAir These birds could fly to South Am...   \n",
       "\n",
       "                                        Cleaned-Text  \\\n",
       "0  americanair why did you drop my call why don t...   \n",
       "1  usairways thanks for the seat that doesn t rec...   \n",
       "2  americanair wasn t just a delay your counter w...   \n",
       "3  united social media team is on point on oscarn...   \n",
       "4  americanair these birds could fly to south ame...   \n",
       "\n",
       "                      Cleaned-Text-StopWords-Removed  \\\n",
       "0  americanair drop call people answering phones ...   \n",
       "1  usairways thanks seat recline shocked asked se...   \n",
       "2  americanair delay counter take valid cac card ...   \n",
       "3          united social media team point oscarnight   \n",
       "4  americanair birds could fly south america exam...   \n",
       "\n",
       "                                 Final-Wrangled-Text  SentimentLabel  \n",
       "0  drop call people answering phones always high ...               0  \n",
       "1  thanks seat recline shocked asked serve everyo...               0  \n",
       "2  delay counter take valid cac card valid id nee...               0  \n",
       "3                 social media team point oscarnight               2  \n",
       "4    birds could fly south america example argentina               1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3d1e7d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VICTUS\\AppData\\Local\\Temp\\ipykernel_21960\\2345969973.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Result['Predicted_sentiment']=list(map(lambda x:GetOriginalSentiment(x),y_pred))\n"
     ]
    }
   ],
   "source": [
    "def  GetOriginalSentiment(val):\n",
    "    if val==0:\n",
    "        return 'negative'\n",
    "    elif val==1:\n",
    "        return 'neutral'\n",
    "    else:\n",
    "        return 'positive'\n",
    "Result=test[['text','airline_sentiment']]\n",
    "Result['Predicted_sentiment']=list(map(lambda x:GetOriginalSentiment(x),y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "1f813c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>Predicted_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@AmericanAir why did you drop my call. Why don...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>@USAirways thanks for the seat that doesn't re...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@AmericanAir wasn't just a delay. Your counter...</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@united social media team is on point on #Osca...</td>\n",
       "      <td>positive</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@AmericanAir These birds could fly to South Am...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text airline_sentiment  \\\n",
       "0  @AmericanAir why did you drop my call. Why don...          negative   \n",
       "1  @USAirways thanks for the seat that doesn't re...          negative   \n",
       "2  @AmericanAir wasn't just a delay. Your counter...          negative   \n",
       "3  @united social media team is on point on #Osca...          positive   \n",
       "4  @AmericanAir These birds could fly to South Am...           neutral   \n",
       "\n",
       "  Predicted_sentiment  \n",
       "0            negative  \n",
       "1            negative  \n",
       "2            negative  \n",
       "3            negative  \n",
       "4             neutral  "
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Result.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96196c7",
   "metadata": {},
   "source": [
    "Print and explain the Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "003d1c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "\n",
      " [[2414   69   25]\n",
      " [ 470  321   60]\n",
      " [ 244   49  348]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Confusion Matrix:\\n\\n\",metrics.confusion_matrix(Result['airline_sentiment'],Result['Predicted_sentiment'],labels=['negative','neutral','positive']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5696abd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain confusion matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4d1c7f72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual negativePredicted negative: 2414\n",
      "Actual negativePredicted neutral: 69\n",
      "Actual negativePredicted positive: 25\n",
      "Actual neutralPredicted negative: 470\n",
      "Actual neutralPredicted neutral: 321\n",
      "Actual neutralPredicted positive: 60\n",
      "Actual positivePredicted negative: 244\n",
      "Actual positivePredicted neutral: 49\n",
      "Actual positivePredicted positive: 348\n"
     ]
    }
   ],
   "source": [
    "for i,x in Result.groupby(['airline_sentiment','Predicted_sentiment']):\n",
    "    print (\"Actual \"+i[0]+ \"Predicted \"+i[1]+\":\",len(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0519e79",
   "metadata": {},
   "source": [
    "Compute Accuracy of your model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d378a1f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy= 77.75 %\n"
     ]
    }
   ],
   "source": [
    "ActualNegativePredictedNegative=2414\n",
    "ActualNeutralPredictedNeutral=321\n",
    "ActualNeutralPredictedNeutral=348\n",
    "TotalCorrect=ActualNegativePredictedNegative+ActualNeutralPredictedNeutral+ActualNeutralPredictedNeutral\n",
    "print(\"Accuracy=\",TotalCorrect*100.0/len(test),\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9631540",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e706c3c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6be40e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
